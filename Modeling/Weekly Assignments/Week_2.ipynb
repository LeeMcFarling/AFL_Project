{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec67006",
   "metadata": {},
   "source": [
    "# **Week 2**\n",
    "\n",
    "Lasso, Ridge, and Elastic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59e6da",
   "metadata": {},
   "source": [
    "#### **Package Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c266f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    RepeatedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, accuracy_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2bec8",
   "metadata": {},
   "source": [
    "#### **Dataset Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e20c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDB_All_Plays_Model_Ready = pd.read_csv(\"../../AFL_Final_Project/BDB_All_Plays_Model_Ready.csv\") # Big Data Bowl Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312430d5",
   "metadata": {},
   "source": [
    "#### **Getting Started: Decisions Made Thus Far**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde51677",
   "metadata": {},
   "source": [
    "The 'Big Data Bowl' dataset is the most difficult of the three football datasets to work with, but it is also the most feature rich. The following is a collection of observations and datapoints made thus far. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f005670",
   "metadata": {},
   "source": [
    "#### **1. Numeric Data Correlation with the target Variable: *Inj_Occurred***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b74932",
   "metadata": {},
   "source": [
    "Bi and multivariate analysis in Semester 2 showed that none of the numeric fields had a strong correlation with the target variable, aside from the foulID columns. These were later encoded into Foul Flag columns (six seperate foul ID columns were present, each containing redundant information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e310e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0972edef",
   "metadata": {},
   "source": [
    "#### **2. Categorical Relation to target Variable**\n",
    "\n",
    "The following columns have low p-vlaues meaning their relationship with the target variable is considered not due to chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63348241",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bbe449b",
   "metadata": {},
   "source": [
    "#### **3. VIF Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f8ebe",
   "metadata": {},
   "source": [
    "The following features were dropped because they had extremely high VIF values, even after multicollinearity drops in dummie variable creation were done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd1a78",
   "metadata": {},
   "source": [
    "- pff_cassCoverage - VIF scores so high it was litterally off the charts\n",
    "- personnelO - VIF scores so high it was literally off the charts\n",
    "- personnelD - Highly correlated with other defensive stats. \n",
    "- prePenaltyPlayResult - similar to play result\n",
    "- defendersInBox -High VIF but not highly correlated with target\n",
    "- quarter - Highly correlated with other time variables but not with target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703dbba1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996494b8",
   "metadata": {},
   "source": [
    "#### **Function Definitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a01b90",
   "metadata": {},
   "source": [
    "Function to take a provided dataframe and split that dataframe into feature and target columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e99be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================\n",
    "# Function taken from Module 3 Final Project\n",
    "# https://github.com/LeeMcFarling/Final_Project_Writeup/blob/main/Final_Project_Report.ipynb\n",
    "# ===========================================================================================\n",
    "\n",
    "def train_test_split_data(df, target_col):\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d1c1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa59b24",
   "metadata": {},
   "source": [
    "#### **Lists to split Data into Numeric and Categorical Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90654dde",
   "metadata": {},
   "source": [
    "Because we already made one hot encoded variables here are lists to seperate numeric and categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    " 'down',\n",
    " 'yardsToGo',\n",
    " 'yardlineNumber',\n",
    " 'preSnapHomeScore',\n",
    " 'preSnapVisitorScore',\n",
    " 'penaltyYards',\n",
    " 'playResult',\n",
    " 'absoluteYardlineNumber',\n",
    " 'frac_quarter_elapsed']\n",
    "\n",
    "categorical_columns = [\n",
    " 'possessionTeam_ATL',\n",
    " 'possessionTeam_BAL',\n",
    " 'possessionTeam_BUF',\n",
    " 'possessionTeam_CAR',\n",
    " 'possessionTeam_CHI',\n",
    " 'possessionTeam_CIN',\n",
    " 'possessionTeam_CLE',\n",
    " 'possessionTeam_DAL',\n",
    " 'possessionTeam_DEN',\n",
    " 'possessionTeam_DET',\n",
    " 'possessionTeam_GB',\n",
    " 'possessionTeam_HOU',\n",
    " 'possessionTeam_IND',\n",
    " 'possessionTeam_JAX',\n",
    " 'possessionTeam_KC',\n",
    " 'possessionTeam_LA',\n",
    " 'possessionTeam_LAC',\n",
    " 'possessionTeam_LV',\n",
    " 'possessionTeam_MIA',\n",
    " 'possessionTeam_MIN',\n",
    " 'possessionTeam_NE',\n",
    " 'possessionTeam_NO',\n",
    " 'possessionTeam_NYG',\n",
    " 'possessionTeam_NYJ',\n",
    " 'possessionTeam_PHI',\n",
    " 'possessionTeam_PIT',\n",
    " 'possessionTeam_SEA',\n",
    " 'possessionTeam_SF',\n",
    " 'possessionTeam_TB',\n",
    " 'possessionTeam_TEN',\n",
    " 'possessionTeam_WAS',\n",
    " 'defensiveTeam_ATL',\n",
    " 'defensiveTeam_BAL',\n",
    " 'defensiveTeam_BUF',\n",
    " 'defensiveTeam_CAR',\n",
    " 'defensiveTeam_CHI',\n",
    " 'defensiveTeam_CIN',\n",
    " 'defensiveTeam_CLE',\n",
    " 'defensiveTeam_DAL',\n",
    " 'defensiveTeam_DEN',\n",
    " 'defensiveTeam_DET',\n",
    " 'defensiveTeam_GB',\n",
    " 'defensiveTeam_HOU',\n",
    " 'defensiveTeam_IND',\n",
    " 'defensiveTeam_JAX',\n",
    " 'defensiveTeam_KC',\n",
    " 'defensiveTeam_LA',\n",
    " 'defensiveTeam_LAC',\n",
    " 'defensiveTeam_LV',\n",
    " 'defensiveTeam_MIA',\n",
    " 'defensiveTeam_MIN',\n",
    " 'defensiveTeam_NE',\n",
    " 'defensiveTeam_NO',\n",
    " 'defensiveTeam_NYG',\n",
    " 'defensiveTeam_NYJ',\n",
    " 'defensiveTeam_PHI',\n",
    " 'defensiveTeam_PIT',\n",
    " 'defensiveTeam_SEA',\n",
    " 'defensiveTeam_SF',\n",
    " 'defensiveTeam_TB',\n",
    " 'defensiveTeam_TEN',\n",
    " 'defensiveTeam_WAS',\n",
    " 'passResult_I',\n",
    " 'passResult_IN',\n",
    " 'passResult_R',\n",
    " 'passResult_S',\n",
    " 'offenseFormation_I_FORM',\n",
    " 'offenseFormation_JUMBO',\n",
    " 'offenseFormation_PISTOL',\n",
    " 'offenseFormation_SHOTGUN',\n",
    " 'offenseFormation_SINGLEBACK',\n",
    " 'offenseFormation_WILDCAT',\n",
    " 'dropBackType_DESIGNED_ROLLOUT_RIGHT',\n",
    " 'dropBackType_DESIGNED_RUN',\n",
    " 'dropBackType_SCRAMBLE',\n",
    " 'dropBackType_SCRAMBLE_ROLLOUT_LEFT',\n",
    " 'dropBackType_SCRAMBLE_ROLLOUT_RIGHT',\n",
    " 'dropBackType_UNKNOWN',\n",
    " 'pff_passCoverageType_Other',\n",
    " 'pff_passCoverageType_Zone',\n",
    " 'pff_playAction',\n",
    " 'Inj_Occured',\n",
    " 'foul_on_play',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133d0dd",
   "metadata": {},
   "source": [
    "#### **Standardization Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================================\n",
    "# Standardize Numeric Features\n",
    "# \n",
    "# Taken from Mod 3 final project found here: \n",
    "# https://github.com/LeeMcFarling/Final_Project_Writeup/blob/main/Final_Project_Report.ipynb\n",
    "#\n",
    "# Note: some errors were calling so I just hard coded it. \n",
    "# =============================================================================================\n",
    "\n",
    "def standardize_features(df, target_column=None, debug=False, return_scaler=False):\n",
    "    df_scaled = df.copy()\n",
    "    numeric_cols = numeric_columns = [\n",
    "                        'down',\n",
    "                        'yardsToGo',\n",
    "                        'yardlineNumber',\n",
    "                        'preSnapHomeScore',\n",
    "                        'preSnapVisitorScore',\n",
    "                        'penaltyYards',\n",
    "                        'playResult',\n",
    "                        'absoluteYardlineNumber',\n",
    "                        'frac_quarter_elapsed']\n",
    "\n",
    "    if target_column in numeric_cols:\n",
    "        numeric_cols.remove(target_column)\n",
    "\n",
    "    # Step 4: Further exclude binary (0/1) columns\n",
    "    numeric_cols_to_scale = [\n",
    "        col for col in numeric_cols\n",
    "        if df[col].nunique(dropna=True) > 2\n",
    "    ]\n",
    "\n",
    "    # DEBUG\n",
    "    if debug:\n",
    "        print(\"Numeric columns before filtering:\", numeric_cols)\n",
    "        print(\"Numeric columns after filtering:\", numeric_cols_to_scale)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[numeric_cols_to_scale] = scaler.fit_transform(df_scaled[numeric_cols_to_scale])\n",
    "\n",
    "    if return_scaler:\n",
    "        return df_scaled, scaler\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a09c98",
   "metadata": {},
   "source": [
    "#### **Run Model Classifier** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================================\n",
    "# Taken from Mod 3 Week 8:\n",
    "# https://github.com/waysnyder/Module-3-Assignments/blob/main/Homework_08.ipynb\n",
    "# =============================================================================================\n",
    "\n",
    "def run_model_classifier(model, X_train, y_train, X_test, y_test, n_repeats=10, n_jobs=-1, **model_params):\n",
    "\n",
    "    # Remove extra key used to store error metric, if it was added to the parameter dictionary\n",
    "    \n",
    "    if 'accuracy_found' in model_params:\n",
    "        model_params = model_params.copy()\n",
    "        model_params.pop('accuracy_found', None)  \n",
    "        \n",
    "    # Instantiate the model if a class is provided\n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "    else:                                    \n",
    "        model.set_params(**model_params)    \n",
    "\n",
    "    # Use RepeatedStratifiedKFold for classification to preserve class distribution\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeats, random_state=42)\n",
    "    \n",
    "    # Perform 5-fold cross-validation using accuracy as the scoring metric\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=n_jobs)\n",
    "    \n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    std_cv_accuracy  = np.std(cv_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training and testing accuracy\n",
    "    train_preds    = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    test_preds     = model.predict(X_test)\n",
    "    test_accuracy  = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    return mean_cv_accuracy, std_cv_accuracy, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868e789",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4f824",
   "metadata": {},
   "source": [
    "#### **Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0feccfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Standardized Data\n",
    "X_train, X_test, y_train, y_test = train_test_split_data(BDB_All_Plays_Model_Ready, 'Inj_Occured')\n",
    "\n",
    "\n",
    "# Standardized Numeric Data\n",
    "BDB_All_Plays_Standardized = standardize_features(BDB_All_Plays_Model_Ready, target_column='Inj_Occured')\n",
    "X_train_Scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split_data(BDB_All_Plays_Standardized, 'Inj_Occured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c693db0",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da11348",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "\n",
    "NOTE: As previously discussed in Semester 2, the primary goal of this analysis and modeling excercise is to classify whether a particular play will result in an injury, and to determine the factors that are most likely to cause this injury. Furthermore, as was *also* previously discussed, this dataset has some extreme imbalance issues (injury occurance < 2%), and as such high performance on these baseline models is NOT expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78002129",
   "metadata": {},
   "source": [
    "## **Baseline Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7280134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Acc:  66.06%\n",
      "Std CV Acc:   0.1612\n",
      "Train Acc:    59.34%\n",
      "Test Acc:     57.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "#\n",
    "# Parameters chosen from SK Learn Documentation and picked for imbalance. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# =============================================================================================\n",
    "\n",
    "params_ = {\n",
    "    'class_weight' : 'balanced',     # attempt to balance dataset\n",
    "    'solver': 'saga',                # Doc said that this solver is better for larger datasets\n",
    "    'penalty': 'l2',                 # default \n",
    "    'max_iter' : 10000,\n",
    "    'fit_intercept': True,\n",
    "    'random_state' : 42\n",
    "}\n",
    "mean_cv_accuracy, std_cv_accuracy, train_accuracy, test_accuracy = run_model_classifier(LogisticRegression, X_train, y_train, X_test, y_test, n_repeats=5, n_jobs=-1, **params_)\n",
    "\n",
    "\n",
    "print(f\"Mean CV Acc:  {mean_cv_accuracy*100:.2f}%\")\n",
    "print(f\"Std CV Acc:   {std_cv_accuracy:.4f}\")\n",
    "print(f\"Train Acc:    {train_accuracy*100:.2f}%\")\n",
    "print(f\"Test Acc:     {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f325713",
   "metadata": {},
   "source": [
    "^ On our first runs, there were a ton of convergence warnings, so the max_iter was cranked up to 50 and tolerance was set at 1e-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5891932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Acc:  64.08%\n",
      "Std CV Acc:   0.0821\n",
      "Train Acc:    63.27%\n",
      "Test Acc:     62.57%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "#\n",
    "# Parameters chosen from SK Learn Documentation and picked for imbalance. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# =============================================================================================\n",
    "\n",
    "params_ = {\n",
    "    'class_weight' : 'balanced',        # attempt to balance dataset\n",
    "    'solver': 'saga',                   # Doc said that this solver is better for larger datasets\n",
    "    'penalty': 'l2',                    # default\n",
    "    'max_iter' : 50000,                 # Iteratively increased this until Convergence Warnings went away\n",
    "    'tol': 1e-2,                        # Another convergence warning measure\n",
    "    'random_state' : 42\n",
    "}\n",
    "mean_cv_accuracy, std_cv_accuracy, train_accuracy, test_accuracy = run_model_classifier(LogisticRegression,\n",
    "                                                                                         X_train, \n",
    "                                                                                         y_train, \n",
    "                                                                                         X_test, \n",
    "                                                                                         y_test,\n",
    "                                                                                         n_repeats=5, \n",
    "                                                                                         n_jobs=-1, \n",
    "                                                                                         **params_\n",
    "                                                                                         )\n",
    "\n",
    "\n",
    "print(f\"Mean CV Acc:  {mean_cv_accuracy*100:.2f}%\")\n",
    "print(f\"Std CV Acc:   {std_cv_accuracy:.4f}\")\n",
    "print(f\"Train Acc:    {train_accuracy*100:.2f}%\")\n",
    "print(f\"Test Acc:     {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e0bdb",
   "metadata": {},
   "source": [
    "Noice, we fixed the convergence issue -- now let's see how using the standardized data effects this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd657d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Acc:  69.84%\n",
      "Std CV Acc:   0.0173\n",
      "Train Acc:    69.73%\n",
      "Test Acc:     68.19%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "#\n",
    "# Parameters chosen from SK Learn Documentation and picked for imbalance. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# =============================================================================================\n",
    "\n",
    "params_ = {\n",
    "    'class_weight' : 'balanced',\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter' : 50000,\n",
    "    'tol': 1e-2,\n",
    "    'random_state' : 42\n",
    "}\n",
    "mean_cv_accuracy, std_cv_accuracy, train_accuracy, test_accuracy = run_model_classifier(LogisticRegression,\n",
    "                                                                                         X_train_Scaled, \n",
    "                                                                                         y_train_scaled,\n",
    "                                                                                         X_test_scaled,  \n",
    "                                                                                         y_test_scaled,\n",
    "                                                                                         n_repeats=5, \n",
    "                                                                                         n_jobs=-1, \n",
    "                                                                                         **params_\n",
    "                                                                                         )\n",
    "\n",
    "\n",
    "print(f\"Mean CV Acc:  {mean_cv_accuracy*100:.2f}%\")\n",
    "print(f\"Std CV Acc:   {std_cv_accuracy:.4f}\")\n",
    "print(f\"Train Acc:    {train_accuracy*100:.2f}%\")\n",
    "print(f\"Test Acc:     {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1364b",
   "metadata": {},
   "source": [
    "Those are some moderate improvement gains there. We can still use a different scoring accuracy metric to see if that will help even more. \n",
    "\n",
    "Below is the same code as was in cell [39], just with 'balanced_accuracy' substituted in the cross validation scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d1a61a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================================\n",
    "# Taken from Mod 3 Week 8:\n",
    "# https://github.com/waysnyder/Module-3-Assignments/blob/main/Homework_08.ipynb\n",
    "# =============================================================================================\n",
    "\n",
    "def run_model_classifier(model, X_train, y_train, X_test, y_test, n_repeats=10, n_jobs=-1, run_comment=None, return_model=False, concat_results=False, **model_params):\n",
    "\n",
    "    global combined_results\n",
    "    # Remove extra key used to store error metric, if it was added to the parameter dictionary\n",
    "    if 'accuracy_found' in model_params:\n",
    "        model_params = model_params.copy()\n",
    "        model_params.pop('accuracy_found', None)  \n",
    "        \n",
    "    # Instantiate the model if a class is provided\n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "    else:                                    \n",
    "        model.set_params(**model_params)    \n",
    "\n",
    "    model_name = model.__name__ if isinstance(model, type) else model.__class__.__name__\n",
    "\n",
    "\n",
    "    # Use RepeatedStratifiedKFold for classification to preserve class distribution\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeats, random_state=42)\n",
    "    \n",
    "    # Perform 5-fold cross-validation using accuracy as the scoring metric\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, scoring='balanced_accuracy', cv=cv, n_jobs=n_jobs)\n",
    "    \n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    std_cv_accuracy  = np.std(cv_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training and testing accuracy\n",
    "    train_preds    = model.predict(X_train)\n",
    "    test_preds     = model.predict(X_test)\n",
    "\n",
    "    # Normal Accuracy \n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    test_accuracy  = accuracy_score(y_test, test_preds)\n",
    "\n",
    "    # Balanced Accuracy Metrics\n",
    "    balanced_train_accuracy = balanced_accuracy_score(y_train, train_preds)\n",
    "    balanced_test_accuracy = balanced_accuracy_score(y_test, test_preds)\n",
    "\n",
    "    results_df = pd.DataFrame([{\n",
    "        'model': model_name, \n",
    "        'model_params': model.get_params(),\n",
    "        'mean_cv_accuracy': mean_cv_accuracy,\n",
    "        'std_cv_accuracy': std_cv_accuracy,\n",
    "        'train_accuracy': train_accuracy, \n",
    "        'test_accuracy': test_accuracy,\n",
    "        'run_comment': run_comment\n",
    "    }])\n",
    "    \n",
    "    if concat_results:\n",
    "        try:\n",
    "            combined_results = pd.concat([combined_results, results_df], ignore_index=True)\n",
    "        except NameError:\n",
    "            combined_results = results_df\n",
    "            \n",
    "    return (results_df, model) if return_model else results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ab8c6",
   "metadata": {},
   "source": [
    "And re-running the same cell as [46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f32bf65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leemcfarling/venvs/tfclean/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Acc:  57.15%\n",
      "Std CV Acc:   0.0320\n",
      "Train Acc:    72.95%\n",
      "Test Acc:     50.77%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "# Parameters chosen from SK Learn Documentation and picked for imbalance. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# =============================================================================================\n",
    "\n",
    "params_ = {\n",
    "    'class_weight' : 'balanced',\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l2',\n",
    "    'max_iter' : 60000,\n",
    "    'tol': 1e-2,\n",
    "    'random_state' : 42\n",
    "}\n",
    "mean_cv_accuracy, std_cv_accuracy, train_accuracy, test_accuracy = run_model_classifier(LogisticRegression,\n",
    "                                                                                         X_train_Scaled, \n",
    "                                                                                         y_train_scaled,\n",
    "                                                                                         X_test_scaled,  \n",
    "                                                                                         y_test_scaled,\n",
    "                                                                                         n_repeats=5, \n",
    "                                                                                         n_jobs=-1, \n",
    "                                                                                         **params_\n",
    "                                                                                         )\n",
    "\n",
    "\n",
    "print(f\"Mean CV Acc:  {mean_cv_accuracy*100:.2f}%\")\n",
    "print(f\"Std CV Acc:   {std_cv_accuracy:.4f}\")\n",
    "print(f\"Train Acc:    {train_accuracy*100:.2f}%\")\n",
    "print(f\"Test Acc:     {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b281aa2",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258abe79",
   "metadata": {},
   "source": [
    "#### **Baseline Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852a25b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba43030",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b7a63",
   "metadata": {},
   "source": [
    "## **Lasso Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ec01b",
   "metadata": {},
   "source": [
    "So there isn't a lasso classification class in SK-Learn. But I saw tutorials for how to use rounding to force it to exhibit classifier-like behavior here: \n",
    "\n",
    "- https://saturncloud.io/blog/python-classification-with-lasso-how-to-predict-classes/#2\n",
    "\n",
    "And then used ChatGPT to help with syntax to build a class that I could pass into our cross validation function here: \n",
    "\n",
    "- https://chatgpt.com/share/68cf6cfd-ff4c-800f-a7ab-ef0d5d769bf9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "354378f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LassoClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, \n",
    "                alpha=1.0,\n",
    "                max_iter=1000,\n",
    "                tol=1e-4,\n",
    "                random_state=42,\n",
    "                threshold=0.5, \n",
    "                **lasso_params):\n",
    "        \"\"\"\n",
    "        Wraps Lasso regression to behave like a classifier.\n",
    "        \n",
    "        alpha: regularization strength for Lasso\n",
    "        threshold: value above which prediction is class 1, else 0\n",
    "        lasso_params: additional parameters passed to sklearn's Lasso\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.threshold = threshold\n",
    "        self.lasso_params = lasso_params\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Instantiate and fit underlying Lasso\n",
    "        self.model = Lasso(alpha=self.alpha, **self.lasso_params)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get continuous predictions and threshold them\n",
    "        y_pred_cont = self.model.predict(X)\n",
    "        return (y_pred_cont >= self.threshold).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Not strictly meaningful for regression, but we can return\n",
    "        probabilities as scaled regression outputs between 0 and 1.\n",
    "        \"\"\"\n",
    "        y_pred_cont = self.model.predict(X)\n",
    "        # clip to [0, 1] range so it behaves like probabilities\n",
    "        y_proba = np.clip(y_pred_cont, 0, 1)\n",
    "        return np.column_stack([1 - y_proba, y_proba])\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.model.coef_\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        return self.model.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -0. -0. -0.  0. -0.  0.\n",
      "  0. -0.  0. -0. -0.  0. -0. -0. -0.  0. -0. -0. -0.  0.  0. -0. -0. -0.\n",
      "  0. -0.  0. -0.  0.  0.  0.  0. -0. -0. -0. -0.  0. -0.  0. -0.  0.  0.\n",
      " -0. -0. -0. -0.  0.  0. -0.  0.  0. -0. -0.  0. -0. -0.  0.  0.  0.  0.\n",
      " -0.  0. -0. -0. -0. -0. -0.  0. -0. -0. -0.  0.  0. -0. -0. -0.  0.  0.\n",
      "  0. -0. -0.  0. -0. -0. -0. -0.  0.  0.  0. -0.  0.  0.  0. -0. -0.  0.\n",
      " -0.  0. -0. -0.  0.  0.  0. -0. -0.  0.  0. -0.  0. -0.  0.]\n",
      "0.024711215089925426\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "# Parameters chosen from SK Learn Documentation and picked for imbalance. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# =============================================================================================\n",
    "\n",
    "params_ = {\n",
    "    'alpha' : 0.1,\n",
    "    'max_iter' : 60000,\n",
    "    'tol': 1e-2,\n",
    "    'random_state' : 42\n",
    "}\n",
    "results_df, Fitted_Lasso = run_model_classifier(\n",
    "    LassoClassifier,\n",
    "    X_train_Scaled, \n",
    "    y_train_scaled,\n",
    "    X_test_scaled,  \n",
    "    y_test_scaled,\n",
    "    n_repeats=5, \n",
    "    n_jobs=-1, \n",
    "    run_comment=None, \n",
    "    return_model=True,\n",
    "    concat_results=False,\n",
    "    **params_\n",
    "    )\n",
    "\n",
    "results_df\n",
    "\n",
    "\n",
    "# print(f\"Mean CV Acc:  {mean_cv_accuracy*100:.2f}%\")\n",
    "# print(f\"Std CV Acc:   {std_cv_accuracy:.4f}\")\n",
    "# print(f\"Train Acc:    {train_accuracy*100:.2f}%\")\n",
    "# print(f\"Test Acc:     {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "59c3eb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>down</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yardsToGo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yardlineNumber</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preSnapHomeScore</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>preSnapVisitorScore</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>dropBackType_SCRAMBLE_ROLLOUT_LEFT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>dropBackType_SCRAMBLE_ROLLOUT_RIGHT</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>dropBackType_UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>pff_passCoverageType_Other</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>pff_passCoverageType_Zone</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 feature  coef\n",
       "0                                   down   0.0\n",
       "1                              yardsToGo   0.0\n",
       "2                         yardlineNumber   0.0\n",
       "3                       preSnapHomeScore   0.0\n",
       "4                    preSnapVisitorScore   0.0\n",
       "..                                   ...   ...\n",
       "118   dropBackType_SCRAMBLE_ROLLOUT_LEFT   0.0\n",
       "119  dropBackType_SCRAMBLE_ROLLOUT_RIGHT  -0.0\n",
       "120                 dropBackType_UNKNOWN   0.0\n",
       "121           pff_passCoverageType_Other  -0.0\n",
       "122            pff_passCoverageType_Zone   0.0\n",
       "\n",
       "[123 rows x 2 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'feature' : X_train_Scaled.columns,\n",
    "    'coef' : Fitted_Lasso.coef_\n",
    "})\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f48b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfclean)",
   "language": "python",
   "name": "tfclean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
